---
title: "Cirta - Particle Type Classification - Part 2/2"
date: 2020-02-15
categories:
  - Data Science
tags: [Zindi Competitions]
header:
    image: "/images/2020-02-15-Particles/banner.png"
excerpt: "Build a machine learning model to help physicists identify particles in images, Deep learning models & best solution"
mathjax: "true"
---

Banner made from a photo by [Moritz Kindler](https://unsplash.com/@moritz_photography) on Unsplash

This is the second and final part of this challenge. In [the first part](https://obrunet.github.io/data%20science/Particles1/) we've prepared the data set and explored it. The goal is to build a machine learning model to help physicists identify particles in images.

---

# Short Introduction

__If you're in a hurry...__

Proposed by [Zindi](https://zindi.africa/competitions/tic-heap-cirta-particle-classification-challenge)

In this challenge we want to build a machine learning model to help us recognize particles. Particles are the tiny constituant of matter generated in a collision between proton bunches at the Large Hadron Collider at CERN. 

Particles are of course of different types and identifying which particle was produced in an extremly important task for particle physicists. 

Our dataset comprises 350 independent simulated events, where each event contains labelled images of particle trajectories. 

A good model assigns the correct particle type to any particle, even the least frequent ones.

Read throught this notebook to discover more about the particles.

---

# Data Preparation

This is the same process as previously done in the 1st part. Let's create a dictionnary to convert the class numbers to their real names and see how many elements each class is made of.

```python
# code to particle name dictionary : 
dic_types = {11: "electron", 13: "muon", 211: "pion", 321: "kaon", 2212: "proton"}
dic_tf = {11: 0, 13: 1, 211: 2, 321: 3, 2212: 4}
y = np.array(pd.DataFrame(y).replace(dic_tf))
np.unique(y, return_counts=True)
```


    (array([0, 1, 2, 3, 4]), array([  3150,    700, 981050, 160300, 114100]))


---

As usual, we have to split or data, so that we can train our model and later test it on two different sets :

```python
from sklearn.model_selection import train_test_split

# preprocess dataset, split into training and test part
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```

    ((1007440, 100), (251860, 100), (1007440, 1), (251860, 1))


```python
nb_classes = len(np.unique(y))
nb_classes
```


    5


# With a Deep Learning model made with Tensorflow

Since the images are only composed of a few pixels, there is no need to use a C.N.N model. In this case, a M.L.P (Multi Layer Perceptron) will do the job.
At first, it is important to convert the target into categorical features.


```python
from tensorflow.keras.utils import to_categorical

y_train_cat = to_categorical(y_train, num_classes=nb_classes, dtype='int32')
y_test_cat = to_categorical(y_test, num_classes=nb_classes, dtype='int32')
y_train_cat.shape, y_test_cat.shape
```



    ((1007440, 5), (251860, 5))

Let's have a look of our target transformed :


```python
y_train_cat
```


    array([[0, 0, 1, 0, 0],
           [0, 0, 1, 0, 0],
           [0, 0, 1, 0, 0],
           ...,
           [0, 0, 1, 0, 0],
           [0, 0, 0, 0, 1],
           [0, 0, 1, 0, 0]], dtype=int32)


The range of the values of our features spans from 0 to 8 :

```python
X.max(), X.min()
```


    (8.0, 0.0)


TThe Neural Network in our case uses gradient descent as an optimization algorithm to find the appropriate weights(w) for each feature.

we are using the gradient descent algorithm to optimize the cost function and update the weights. So if some of the features have a very large scale then gradient descent takes more numbers of iterations for that feature to converge as compared to the other features having a small scale and if the feature which has high scale is highly correlated to the target output then the performance of the model will be affected as a whole because the weights for this particular feature will not converge to give the best performance. 


```python
#from sklearn.preprocessing import StandardScaler

#scaler = StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.transform(X_test)

X_train, X_test = X_train / 8, X_test / 8
```

# Base line / first attempt


```python
freq_array = np.unique(y, return_counts=True)
class_weight = {k: d for k, d in zip(freq_array[0], freq_array[1])}
class_weight
```


    {0: 3150, 1: 700, 2: 981050, 3: 160300, 4: 114100}




```python
from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y.flatten())
class_weights
```


    array([7.99555556e+01, 3.59800000e+02, 2.56724938e-01, 1.57117904e+00,
           2.20736196e+00])


```python
initial_bias = np.log([700/981050])
initial_bias
```


    array([-7.24529837])


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense


input_dim = X_train.shape[1]

model = Sequential()
model.add(Dense(20, input_dim=input_dim, activation='relu'))#, use_bias = True, bias_initializer='zeros'))
model.add(Dense(10, activation='relu'))
model.add(Dense(nb_classes, activation='softmax'))


model.compile(loss='sparse_categorical_crossentropy',
              optimizer='SGD', # nadam
              metrics=['accuracy'])
```


```python
model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_15 (Dense)             (None, 20)                2020      
    _________________________________________________________________
    dense_16 (Dense)             (None, 10)                210       
    _________________________________________________________________
    dense_17 (Dense)             (None, 5)                 55        
    =================================================================
    Total params: 2,285
    Trainable params: 2,285
    Non-trainable params: 0
    _________________________________________________________________
    


```python
#model.get_weights()
```


```python
y_train_cat.shape
```


    (1007440, 5)




```python
model.fit(X_train, y_train,
          class_weight=class_weight,
          epochs=8, 
          batch_size=1000, 
          validation_data=(X_test, y_test))
```

    Train on 1007440 samples, validate on 251860 samples
    Epoch 1/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6918 - acc: 0.7790 - val_loss: 0.6917 - val_acc: 0.7790
    Epoch 2/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6916 - acc: 0.7790 - val_loss: 0.6916 - val_acc: 0.7790
    Epoch 3/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6915 - acc: 0.7790 - val_loss: 0.6915 - val_acc: 0.7790
    Epoch 4/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6914 - acc: 0.7790 - val_loss: 0.6913 - val_acc: 0.7790
    Epoch 5/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6913 - acc: 0.7790 - val_loss: 0.6912 - val_acc: 0.7790
    Epoch 6/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6912 - acc: 0.7790 - val_loss: 0.6911 - val_acc: 0.7790
    Epoch 7/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6911 - acc: 0.7790 - val_loss: 0.6910 - val_acc: 0.7790
    Epoch 8/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6909 - acc: 0.7790 - val_loss: 0.6909 - val_acc: 0.7790




```python
y_pred_train = model.predict(X_train, batch_size=1000)
```


```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_train.flatten(), np.argmax(y_pred_train, axis=1))
```


    array([[     0,      0,   2520,      0,      0],
           [     0,      0,    560,      0,      0],
           [     0,      0, 784840,      0,      0],
           [     0,      0, 128240,      0,      0],
           [     0,      0,  91280,      0,      0]])



```python
from sklearn.metrics import confusion_matrix, log_loss


import itertools    
class_names = ['11', '13', '211', '321', '2212']


def plot_confusion_mtx(trained_model):
    """Plot the confusion matrix with color and labels"""
    matrix = confusion_matrix(y_test.flatten(),
                              np.argmax(trained_model.predict(X_test, batch_size=50000),
                              axis=1))
    plt.clf()

    # place labels at the top
    plt.gca().xaxis.tick_top()
    plt.gca().xaxis.set_label_position('top')

    # plot the matrix per se
    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)

    # plot colorbar to the right
    plt.colorbar()
    fmt = 'd'

    # write the number of predictions in each bucket
    thresh = matrix.max() / 2.
    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):

        # if background is dark, use a white number, and vice-versa
        plt.text(j, i, format(matrix[i, j], fmt),
             horizontalalignment="center",
             color="white" if matrix[i, j] > thresh else "black")

    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)
    plt.tight_layout()
    plt.ylabel('True label',size=14)
    plt.xlabel('Predicted label',size=14)
    plt.show()
    
    
def display_confusion_matx(best_model_path):
    if os.path.isfile(best_model_path):
        with open(best_model_path, 'rb') as f:
            best_clf = pickle.load(f)
        plot_confusion_mtx(best_clf)
        
        
#display_confusion_matx('1_original_data_LGBMClassifier')
```


```python
np.unique(y_pred, return_counts=True)
```


    (array([0, 2]), array([3, 3]))



```python
plot_confusion_mtx(model)
```


![png](/images/2020-02-15-Particles/output_29_0.png)

# Second Try with other parameters


Using a different model


```python
model = Sequential()
input_dim = X_train.shape[1]
nb_classes = y_train.shape[1]

model.add(Dense(10, input_dim=input_dim, activation='relu', name='input'))
model.add(Dense(20, activation='relu', name='fc1'))
model.add(Dense(10, activation='relu', name='fc2'))
model.add(Dense(nb_classes, activation='softmax', name='output'))

model.compile(loss='categorical_crossentropy',
              optimizer='nadam',
              metrics=['accuracy'])


# from sklearn.utils import class_weight
# class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
#class_weight = {0 : 1., 1: 20.}
model.fit(X_train, y_train, epochs=3, batch_size=50000, class_weight={0: 3150, 1: 700, 2: 981050, 3: 160300, 4: 114100})
```

    WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Epoch 1/3
    1007440/1007440 [==============================] - 3s 3us/sample - loss: 1.5577 - acc: 0.4882
    Epoch 2/3
    1007440/1007440 [==============================] - 3s 3us/sample - loss: 1.1783 - acc: 0.7790
    Epoch 3/3
    1007440/1007440 [==============================] - 3s 3us/sample - loss: 0.5487 - acc: 0.7790
    


```python
score = model.evaluate(X_test, y_test, batch_size=50000)
score
```

    251860/251860 [==============================] - 0s 1us/sample - loss: 1.0910 - acc: 0.7790

    [1.0910033097309182, 0.7790439]


Unfortunately, the results were the same. The predictions weren't improved !

