---
title: "Cirta - Particle Type Classification - Part 2/2"
date: 2020-02-15
categories:
  - Data Science
tags: [Zindi Competitions]
header:
    image: "/images/2020-02-15-Particles/banner.png"
excerpt: "Build a machine learning model to help physicists identify particles in images, Deep learning models & best solution"
mathjax: "true"
---

Banner made from a photo by [Moritz Kindler](https://unsplash.com/@moritz_photography) on Unsplash

This is the second and final part of this challenge. In [the first part](https://obrunet.github.io/data%20science/Particles1/) we've prepared the data set and explored it. The goal is to build a machine learning model to help physicists identify particles in images.

---

# Short Introduction

__If you're in a hurry...__

Proposed by [Zindi](https://zindi.africa/competitions/tic-heap-cirta-particle-classification-challenge)

In this challenge we want to build a machine learning model to help us recognize particles. Particles are the tiny constituant of matter generated in a collision between proton bunches at the Large Hadron Collider at CERN. 

Particles are of course of different types and identifying which particle was produced in an extremly important task for particle physicists. 

Our dataset comprises 350 independent simulated events, where each event contains labelled images of particle trajectories. 

A good model assigns the correct particle type to any particle, even the least frequent ones.

Read throught this notebook to discover more about the particles.

---




# Tensorflow

```python
np.unique(y, return_counts=True)
```




    (array([  11,   13,  211,  321, 2212]),
     array([  3150,    700, 981050, 160300, 114100]))




```python
# code to particle name dictionary : 
dic_types = {11: "electron", 13: "muon", 211: "pion", 321: "kaon", 2212: "proton"}
dic_tf = {11: 0, 13: 1, 211: 2, 321: 3, 2212: 4}
y = np.array(pd.DataFrame(y).replace(dic_tf))
np.unique(y, return_counts=True)
```




    (array([0, 1, 2, 3, 4]), array([  3150,    700, 981050, 160300, 114100]))



---
# Data Prep


```python
from sklearn.model_selection import train_test_split

# preprocess dataset, split into training and test part
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```




    ((1007440, 100), (251860, 100), (1007440, 1), (251860, 1))




```python
nb_classes = len(np.unique(y))
nb_classes
```




    5




```python
from tensorflow.keras.utils import to_categorical

y_train_cat = to_categorical(y_train, num_classes=nb_classes, dtype='int32')
y_test_cat = to_categorical(y_test, num_classes=nb_classes, dtype='int32')
y_train_cat.shape, y_test_cat.shape
```




    ((1007440, 5), (251860, 5))




```python
y_train_cat
```




    array([[0, 0, 1, 0, 0],
           [0, 0, 1, 0, 0],
           [0, 0, 1, 0, 0],
           ...,
           [0, 0, 1, 0, 0],
           [0, 0, 0, 0, 1],
           [0, 0, 1, 0, 0]], dtype=int32)




```python
X.max(), X.min()
```




    (8.0, 0.0)




```python
#from sklearn.preprocessing import StandardScaler

#scaler = StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.transform(X_test)

X_train, X_test = X_train / 8, X_test / 8
```


```python
np.unique(y_train, return_counts=True)
```




    (array([0, 1, 2, 3, 4]), array([  2520,    560, 784840, 128240,  91280]))




```python
np.unique(y_test, return_counts=True)
```




    (array([0, 1, 2, 3, 4]), array([   630,    140, 196210,  32060,  22820]))



# Base line


```python
freq_array = np.unique(y, return_counts=True)
class_weight = {k: d for k, d in zip(freq_array[0], freq_array[1])}
class_weight
```




    {0: 3150, 1: 700, 2: 981050, 3: 160300, 4: 114100}




```python
from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y.flatten())
class_weights
```




    array([7.99555556e+01, 3.59800000e+02, 2.56724938e-01, 1.57117904e+00,
           2.20736196e+00])




```python
initial_bias = np.log([700/981050])
initial_bias
```




    array([-7.24529837])




```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense


input_dim = X_train.shape[1]

model = Sequential()
model.add(Dense(20, input_dim=input_dim, activation='relu'))#, use_bias = True, bias_initializer='zeros'))
model.add(Dense(10, activation='relu'))
model.add(Dense(nb_classes, activation='softmax'))


model.compile(loss='sparse_categorical_crossentropy',
              optimizer='SGD', # nadam
              metrics=['accuracy'])
```


```python
model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_15 (Dense)             (None, 20)                2020      
    _________________________________________________________________
    dense_16 (Dense)             (None, 10)                210       
    _________________________________________________________________
    dense_17 (Dense)             (None, 5)                 55        
    =================================================================
    Total params: 2,285
    Trainable params: 2,285
    Non-trainable params: 0
    _________________________________________________________________
    


```python
#model.get_weights()
```


```python
y_train_cat.shape
```




    (1007440, 5)




```python
model.fit(X_train, y_train,
          class_weight=class_weight,
          epochs=8, 
          batch_size=1000, 
          validation_data=(X_test, y_test))
```

    Train on 1007440 samples, validate on 251860 samples
    Epoch 1/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6918 - acc: 0.7790 - val_loss: 0.6917 - val_acc: 0.7790
    Epoch 2/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6916 - acc: 0.7790 - val_loss: 0.6916 - val_acc: 0.7790
    Epoch 3/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6915 - acc: 0.7790 - val_loss: 0.6915 - val_acc: 0.7790
    Epoch 4/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6914 - acc: 0.7790 - val_loss: 0.6913 - val_acc: 0.7790
    Epoch 5/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6913 - acc: 0.7790 - val_loss: 0.6912 - val_acc: 0.7790
    Epoch 6/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6912 - acc: 0.7790 - val_loss: 0.6911 - val_acc: 0.7790
    Epoch 7/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6911 - acc: 0.7790 - val_loss: 0.6910 - val_acc: 0.7790
    Epoch 8/8
    1007440/1007440 [==============================] - 2s 2us/sample - loss: 0.6909 - acc: 0.7790 - val_loss: 0.6909 - val_acc: 0.7790
    




    <tensorflow.python.keras.callbacks.History at 0x7f648aa8a2e8>




```python
y_pred_train = model.predict(X_train, batch_size=1000)
```


```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_train.flatten(), np.argmax(y_pred_train, axis=1))
```




    array([[     0,      0,   2520,      0,      0],
           [     0,      0,    560,      0,      0],
           [     0,      0, 784840,      0,      0],
           [     0,      0, 128240,      0,      0],
           [     0,      0,  91280,      0,      0]])



# Metric


```python
np.unique(y_test, return_counts=True)
```




    (array([0, 1, 2, 3, 4]), array([   630,    140, 196210,  32060,  22820]))




```python
np.unique(X_train, return_counts=True)
```




    (array([0.   , 0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875, 1.   ]),
     array([93266949,  4772426,  1724837,   652990,   226204,    69197,
               28314,     2799,      284]))




```python
np.argmax(y_pred_train, axis=1)
```




    array([2, 2, 2, ..., 2, 2, 2])




```python
y_train.flatten()
```




    array([2, 2, 2, ..., 2, 4, 2])




```python
from sklearn.metrics import confusion_matrix, log_loss


import itertools    
class_names = ['11', '13', '211', '321', '2212']


def plot_confusion_mtx(trained_model):
    """Plot the confusion matrix with color and labels"""
    matrix = confusion_matrix(y_test.flatten(),
                              np.argmax(trained_model.predict(X_test, batch_size=50000),
                              axis=1))
    plt.clf()

    # place labels at the top
    plt.gca().xaxis.tick_top()
    plt.gca().xaxis.set_label_position('top')

    # plot the matrix per se
    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)

    # plot colorbar to the right
    plt.colorbar()
    fmt = 'd'

    # write the number of predictions in each bucket
    thresh = matrix.max() / 2.
    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):

        # if background is dark, use a white number, and vice-versa
        plt.text(j, i, format(matrix[i, j], fmt),
             horizontalalignment="center",
             color="white" if matrix[i, j] > thresh else "black")

    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)
    plt.tight_layout()
    plt.ylabel('True label',size=14)
    plt.xlabel('Predicted label',size=14)
    plt.show()
    
    
def display_confusion_matx(best_model_path):
    if os.path.isfile(best_model_path):
        with open(best_model_path, 'rb') as f:
            best_clf = pickle.load(f)
        plot_confusion_mtx(best_clf)
        
        
#display_confusion_matx('1_original_data_LGBMClassifier')
```


```python
np.unique(y_pred, return_counts=True)
```




    (array([0, 2]), array([3, 3]))




```python
plot_confusion_mtx(model)
```


![png](/images/2020-02-15-Particles/output_29_0.png)

# tensorflow with other param


Using a different model


```python
model = Sequential()
input_dim = X_train.shape[1]
nb_classes = y_train.shape[1]

model.add(Dense(10, input_dim=input_dim, activation='relu', name='input'))
model.add(Dense(20, activation='relu', name='fc1'))
model.add(Dense(10, activation='relu', name='fc2'))
model.add(Dense(nb_classes, activation='softmax', name='output'))

model.compile(loss='categorical_crossentropy',
              optimizer='nadam',
              metrics=['accuracy'])
```

    WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.
    


```python
# from sklearn.utils import class_weight
# class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)
#class_weight = {0 : 1., 1: 20.}
model.fit(X_train, y_train, epochs=3, batch_size=50000, class_weight={0: 3150, 1: 700, 2: 981050, 3: 160300, 4: 114100})
```

    WARNING:tensorflow:From C:\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Epoch 1/3
    1007440/1007440 [==============================] - 3s 3us/sample - loss: 1.5577 - acc: 0.4882
    Epoch 2/3
    1007440/1007440 [==============================] - 3s 3us/sample - loss: 1.1783 - acc: 0.7790
    Epoch 3/3
    1007440/1007440 [==============================] - 3s 3us/sample - loss: 0.5487 - acc: 0.7790
    




    <tensorflow.python.keras.callbacks.History at 0x212810bd2b0>




```python
score = model.evaluate(X_test, y_test, batch_size=50000)
score
```

    251860/251860 [==============================] - 0s 1us/sample - loss: 1.0910 - acc: 0.7790
    




    [1.0910033097309182, 0.7790439]




```python
%matplotlib inline
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(font_scale=2)
predictions = model.predict(X_test, batch_size=50000)

LABELS = ['Normal','Fraud'] 

max_test = np.argmax(y_test, axis=1)
max_predictions = np.argmax(predictions, axis=1)
confusion_matrix = metrics.confusion_matrix(max_test, max_predictions)

plt.figure(figsize=(5, 5))
sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d", annot_kws={"size": 20});
plt.title("Confusion matrix", fontsize=20)
plt.ylabel('True label', fontsize=20)
plt.xlabel('Predicted label', fontsize=20)
plt.show()
```


![png](/images/2020-02-15-Particles/output_4_0.png)

