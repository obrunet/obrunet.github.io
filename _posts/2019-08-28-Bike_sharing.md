---
title: "Forecast of city bikeshare system's use / rentals"
date: 2019-08-28
categories:
  - Data Science
tags: [Kaggle Competitions]
header:
    image: "/images/2019-08-28-Bike_sharing/christian-stahl-xmGkzY--Fgg-unsplash.jpg"
excerpt: "Data Science challenge - Prediction of renting bicycles in Washington D.C"
mathjax: "true"
---



Banner photo by [Christian Stahl](https://unsplash.com/@woodpecker65)


## Context

Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.

The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.


## Goal
Forecast use of a city bikeshare system

---

# Exploratory Data Analysis


```python
import numpy as np
import pandas as pd
from scipy import stats
import seaborn as sns
import matplotlib.pyplot as plt

pd.options.display.max_columns = 100

import warnings
warnings.filterwarnings("ignore")
```


```python
from sklearn.svm import LinearSVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
```


```python
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
```


```python
import xgboost as xgb
import lightgbm as lgbm
```

Let's see the head of the data set


```python
df = pd.read_csv("../input/train.csv")
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-01 00:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>81</td>
      <td>0.0</td>
      <td>3</td>
      <td>13</td>
      <td>16</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-01 01:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>8</td>
      <td>32</td>
      <td>40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-01 02:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>5</td>
      <td>27</td>
      <td>32</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-01 03:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-01 04:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



The dataset contains the following columns:

```
datetime - hourly date + timestamp  
season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
holiday - whether the day is considered a holiday
workingday - whether the day is neither a weekend nor holiday
weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy 
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist 
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
temp - temperature in Celsius
atemp - "feels like" temperature in Celsius
humidity - relative humidity
windspeed - wind speed
casual - number of non-registered user rentals initiated
registered - number of registered user rentals initiated
count - number of total rentals
```

'count' is the target, all other columns are features. 

The value of 'count' is equal to casual plus register


```python
df.shape
```




    (10886, 12)




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 10886 entries, 0 to 10885
    Data columns (total 12 columns):
    datetime      10886 non-null object
    season        10886 non-null int64
    holiday       10886 non-null int64
    workingday    10886 non-null int64
    weather       10886 non-null int64
    temp          10886 non-null float64
    atemp         10886 non-null float64
    humidity      10886 non-null int64
    windspeed     10886 non-null float64
    casual        10886 non-null int64
    registered    10886 non-null int64
    count         10886 non-null int64
    dtypes: float64(3), int64(8), object(1)
    memory usage: 1020.6+ KB


There are basiclly 3 types of data:
- data concerning the weather
- for time infos
- linked to the number of users

No Nan, it seems that there isn't any missing value. All types seem to be corresponding to the meaning of the data except the 'datetime' column wich is an object not a pandas' datetime
Let's see the basic statistics:


```python
df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.00000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
      <td>10886.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.506614</td>
      <td>0.028569</td>
      <td>0.680875</td>
      <td>1.418427</td>
      <td>20.23086</td>
      <td>23.655084</td>
      <td>61.886460</td>
      <td>12.799395</td>
      <td>36.021955</td>
      <td>155.552177</td>
      <td>191.574132</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.116174</td>
      <td>0.166599</td>
      <td>0.466159</td>
      <td>0.633839</td>
      <td>7.79159</td>
      <td>8.474601</td>
      <td>19.245033</td>
      <td>8.164537</td>
      <td>49.960477</td>
      <td>151.039033</td>
      <td>181.144454</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.82000</td>
      <td>0.760000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>13.94000</td>
      <td>16.665000</td>
      <td>47.000000</td>
      <td>7.001500</td>
      <td>4.000000</td>
      <td>36.000000</td>
      <td>42.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>20.50000</td>
      <td>24.240000</td>
      <td>62.000000</td>
      <td>12.998000</td>
      <td>17.000000</td>
      <td>118.000000</td>
      <td>145.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>26.24000</td>
      <td>31.060000</td>
      <td>77.000000</td>
      <td>16.997900</td>
      <td>49.000000</td>
      <td>222.000000</td>
      <td>284.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>4.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>41.00000</td>
      <td>45.455000</td>
      <td>100.000000</td>
      <td>56.996900</td>
      <td>367.000000</td>
      <td>886.000000</td>
      <td>977.000000</td>
    </tr>
  </tbody>
</table>
</div>



At first glance, when we look at the temp / atemp min, it seems strange that no negative temperature is recorded...

## Weather informations analysis


```python
plt.figure(figsize=(8, 4))
sns.distplot(df.temp, bins=10, label='real temp.')
sns.distplot(df.atemp, bins=10, label='feels like temp.')
plt.legend()
plt.show()
```

![png](/images/2019-08-28-Bike_sharing/output_15_0.png)


One can see an offset between the real temperature and the "feels like" temperature. This can probably explained by the fact that temperature on a bike is different. But the distributions looks the same. More, there is a clear correlation between the 2 features.


```python
plt.figure(figsize=(4, 4))
sns.scatterplot(df.atemp, df.temp)
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_17_0.png)


Let's see if the difference between the 2 features is quite the same...


```python
df['Delta'] = df.temp - df.atemp
sns.lineplot(x=df.index, y=df.Delta)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7fa54c4cf588>




![png](/images/2019-08-28-Bike_sharing/output_19_1.png)


There are few abnormal values corresponding to the spike in the above plot.


```python
df[df.Delta > 5].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
      <th>Delta</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8991</th>
      <td>2012-08-17 00:00:00</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>27.88</td>
      <td>12.12</td>
      <td>57</td>
      <td>11.0014</td>
      <td>21</td>
      <td>67</td>
      <td>88</td>
      <td>15.76</td>
    </tr>
    <tr>
      <th>8992</th>
      <td>2012-08-17 01:00:00</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>27.06</td>
      <td>12.12</td>
      <td>65</td>
      <td>7.0015</td>
      <td>16</td>
      <td>38</td>
      <td>54</td>
      <td>14.94</td>
    </tr>
    <tr>
      <th>8993</th>
      <td>2012-08-17 02:00:00</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>27.06</td>
      <td>12.12</td>
      <td>61</td>
      <td>8.9981</td>
      <td>4</td>
      <td>15</td>
      <td>19</td>
      <td>14.94</td>
    </tr>
    <tr>
      <th>8994</th>
      <td>2012-08-17 03:00:00</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>26.24</td>
      <td>12.12</td>
      <td>65</td>
      <td>7.0015</td>
      <td>0</td>
      <td>6</td>
      <td>6</td>
      <td>14.12</td>
    </tr>
    <tr>
      <th>8995</th>
      <td>2012-08-17 04:00:00</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>26.24</td>
      <td>12.12</td>
      <td>73</td>
      <td>11.0014</td>
      <td>0</td>
      <td>9</td>
      <td>9</td>
      <td>14.12</td>
    </tr>
  </tbody>
</table>
</div>




```python
df[(df.atemp >= 12) & (df.atemp <= 12.5)].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
      <th>Delta</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>59</th>
      <td>2011-01-03 14:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>12.12</td>
      <td>30</td>
      <td>19.0012</td>
      <td>11</td>
      <td>66</td>
      <td>77</td>
      <td>-1.46</td>
    </tr>
    <tr>
      <th>60</th>
      <td>2011-01-03 15:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>12.12</td>
      <td>30</td>
      <td>16.9979</td>
      <td>14</td>
      <td>58</td>
      <td>72</td>
      <td>-1.46</td>
    </tr>
    <tr>
      <th>61</th>
      <td>2011-01-03 16:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>12.12</td>
      <td>30</td>
      <td>16.9979</td>
      <td>9</td>
      <td>67</td>
      <td>76</td>
      <td>-1.46</td>
    </tr>
    <tr>
      <th>109</th>
      <td>2011-01-05 18:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>9.84</td>
      <td>12.12</td>
      <td>38</td>
      <td>8.9981</td>
      <td>3</td>
      <td>166</td>
      <td>169</td>
      <td>-2.28</td>
    </tr>
    <tr>
      <th>115</th>
      <td>2011-01-06 00:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>7.38</td>
      <td>12.12</td>
      <td>55</td>
      <td>0.0000</td>
      <td>0</td>
      <td>11</td>
      <td>11</td>
      <td>-4.74</td>
    </tr>
  </tbody>
</table>
</div>



All those measures have been recorded the same day. During that day 'temp' changed but not 'atemp' The 'atemp' value seems strange even during other few days... Because those 2 features are correlated, we only need to keep one. This is more safe to keep temp instead of atemp

Is there also a correlation between humidity and windspeed ?


```python
plt.figure(figsize=(8, 4))
sns.distplot(df.humidity, bins=10, label='humidity')
sns.distplot(df.windspeed, bins=10, label='windspeed')
plt.legend()
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_25_0.png)



```python
sns.pairplot(df[['temp', 'atemp', 'humidity', 'windspeed']])
```




    <seaborn.axisgrid.PairGrid at 0x7fa547823a58>




![png](/images/2019-08-28-Bike_sharing/output_26_1.png)


Except for the temperature, there is no clear correlation between other features. Furthermore:
- humidity vs atemp there is something strange with missing value like a void in the scatter
- for all windspeed plots there are null values and beginning values start at 5, that's probably because the sensor can detect low values...this needs to be investigated further.

## Modification of time data


```python
df['casual_percentage'] = df['casual'] / df['count']
df['registered_percentage'] = df['registered'] / df['count']
```


```python
def change_datetime(df):
    """ Modify the col datetime to create other cols: dow, month, week..."""
    df["datetime"] = pd.to_datetime(df["datetime"])
    df["dow"] = df["datetime"].dt.dayofweek
    df["month"] = df["datetime"].dt.month
    df["week"] = df["datetime"].dt.week
    df["hour"] = df["datetime"].dt.hour
    df["year"] = df["datetime"].dt.year
    df["season"] = df.season.map({1: "Winter", 2 : "Spring", 3 : "Summer", 4 :"Fall" })
    df["month_str"] = df.month.map({1: "Jan ", 2 : "Feb", 3 : "Mar", 4 : "Apr", 5: "May", 6: "Jun", 7: "Jul", 8: "Aug", 9: "Sep", 10: "Oct", 11: "Nov", 12: "Dec" })
    df["dow_str"] = df.dow.map({5: "Sat", 6 : "Sun", 0 : "Mon", 1 :"Tue", 2 : "Wed", 3 : "Thu", 4: "Fri" })
    df["weather_str"] = df.weather.map({1: "Good", 2 : "Normal", 3 : "Bad", 4 :"Very Bad"})
    return df
    
    
df = change_datetime(df)
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
      <th>Delta</th>
      <th>casual_percentage</th>
      <th>registered_percentage</th>
      <th>dow</th>
      <th>month</th>
      <th>week</th>
      <th>hour</th>
      <th>year</th>
      <th>month_str</th>
      <th>dow_str</th>
      <th>weather_str</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-01 00:00:00</td>
      <td>Winter</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>81</td>
      <td>0.0</td>
      <td>3</td>
      <td>13</td>
      <td>16</td>
      <td>-4.555</td>
      <td>0.187500</td>
      <td>0.812500</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>0</td>
      <td>2011</td>
      <td>Jan</td>
      <td>Sat</td>
      <td>Good</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-01 01:00:00</td>
      <td>Winter</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>8</td>
      <td>32</td>
      <td>40</td>
      <td>-4.615</td>
      <td>0.200000</td>
      <td>0.800000</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>1</td>
      <td>2011</td>
      <td>Jan</td>
      <td>Sat</td>
      <td>Good</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-01 02:00:00</td>
      <td>Winter</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>5</td>
      <td>27</td>
      <td>32</td>
      <td>-4.615</td>
      <td>0.156250</td>
      <td>0.843750</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>2</td>
      <td>2011</td>
      <td>Jan</td>
      <td>Sat</td>
      <td>Good</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-01 03:00:00</td>
      <td>Winter</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>13</td>
      <td>-4.555</td>
      <td>0.230769</td>
      <td>0.769231</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>3</td>
      <td>2011</td>
      <td>Jan</td>
      <td>Sat</td>
      <td>Good</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-01 04:00:00</td>
      <td>Winter</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>-4.555</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>4</td>
      <td>2011</td>
      <td>Jan</td>
      <td>Sat</td>
      <td>Good</td>
    </tr>
  </tbody>
</table>
</div>



## Rentals / target analysis


```python
sns.kdeplot(data=df['count'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7fa5472d17b8>




![png](/images/2019-08-28-Bike_sharing/output_32_1.png)



```python
df['y_log'] = np.log(df['count'])
sns.kdeplot(data=df['y_log'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7fa5452d5e80>




![png](/images/2019-08-28-Bike_sharing/output_33_1.png)



```python
plt.figure(figsize=(12, 6))
sns.pointplot(x=df["hour"], y=df["count"], hue=df["season"])
plt.xlabel("Hour Of The Day")
plt.ylabel("Users Count") 
plt.title("Rentals Across Hours")
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_34_0.png)



```python
plt.figure(figsize=(12, 6))
sns.lineplot(x="hour", y="count", hue="season", data=df)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7fa5452ca518>




![png](/images/2019-08-28-Bike_sharing/output_35_1.png)



```python
# ---------------------------------------------------------
plt.figure(figsize=(6,3))
plt.stackplot(range(1,25),
              df.groupby(['hour'])['casual_percentage'].mean(), 
              df.groupby(['hour'])['registered_percentage'].mean(), 
              labels=['Casual','Registered'])
plt.legend(loc='upper left')
plt.margins(0,0)
plt.title("Evolution of casual /registered bikers' share over hours of the day")

# ---------------------------------------------------------
plt.figure(figsize=(6,6))
df_hours = pd.DataFrame(
    {"casual" : df.groupby(['hour'])['casual'].mean().values,
    "registered" : df.groupby(['hour'])['registered'].mean().values},
    index = df.groupby(['hour'])['casual'].mean().index)
df_hours.plot.bar(rot=0)
plt.title("Evolution of casual /registered bikers numbers over hours of the day")

# ---------------------------------------------------------
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_36_0.png)



    <Figure size 432x432 with 0 Axes>



![png](/images/2019-08-28-Bike_sharing/output_36_2.png)



```python
plt.figure(figsize=(12, 6))
sns.pointplot(x=df["dow"], y=df["count"], hue=df["season"])
plt.xlabel("Day of the week")
plt.ylabel("Users Count") 
plt.title("Rentals Across week days")
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_37_0.png)



```python
# ---------------------------------------------------------
plt.figure(figsize=(6,3))
plt.stackplot(range(1,8),
              df.groupby(['dow'])['casual_percentage'].mean(), 
              df.groupby(['dow'])['registered_percentage'].mean(), 
              labels=['Casual','Registered'])
plt.legend(loc='upper left')
plt.margins(0,0)
plt.title("Evolution of casual /registered bikers' share over weekdays")

# ---------------------------------------------------------
plt.figure(figsize=(6,6))
df_hours = pd.DataFrame(
    {"casual" : df.groupby(['dow'])['casual'].mean().values,
    "registered" : df.groupby(['dow'])['registered'].mean().values},
    index = df.groupby(['dow'])['casual'].mean().index)
df_hours.plot.bar(rot=0)
plt.title("Evolution of casual /registered bikers numbers over weekdays")

# ---------------------------------------------------------
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_38_0.png)



    <Figure size 432x432 with 0 Axes>



![png](/images/2019-08-28-Bike_sharing/output_38_2.png)



```python
fig, ax = plt.subplots()
fig.set_size_inches(10, 8)
sns.boxplot(data=df, y="count", x="month_str", orient="v")
ax.set(xlabel="Months" , ylabel="Count", title="Count Across Month");
```


![png](/images/2019-08-28-Bike_sharing/output_39_0.png)



```python
sns.swarmplot(x='hour', y='temp', data=df, hue='season')
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_40_0.png)



```python
# ---------------------------------------------------------
plt.figure(figsize=(6,3))
plt.stackplot(range(1,13),
              df.groupby(['month'])['casual_percentage'].mean(), 
              df.groupby(['month'])['registered_percentage'].mean(), 
              labels=['Casual','Registered'])
plt.legend(loc='upper left')
plt.margins(0,0)
plt.title("Evolution of casual /registered bikers' share over months of the year")

# ---------------------------------------------------------
plt.figure(figsize=(6,6))
df_hours = pd.DataFrame(
    {"casual" : df.groupby(['month'])['casual'].mean().values,
    "registered" : df.groupby(['month'])['registered'].mean().values},
    index = df.groupby(['month'])['casual'].mean().index)
df_hours.plot.bar(rot=0)
plt.title("Evolution of casual /registered bikers numbers over months of the year")

# ---------------------------------------------------------
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_41_0.png)



    <Figure size 432x432 with 0 Axes>



![png](/images/2019-08-28-Bike_sharing/output_41_2.png)



```python
plt.figure(figsize=(10, 5))

bars = ['casual not on working days', 'casual on working days',\
        'registered not on working days', 'registered on working days',\
        'casual not on holidays', 'casual on holidays',\
        'registered not on holidays', 'registered on holidays']

qty = [df.groupby(['workingday'])['casual'].mean()[0], df.groupby(['workingday'])['casual'].mean()[1],\
      df.groupby(['workingday'])['registered'].mean()[0], df.groupby(['workingday'])['registered'].mean()[1],\
      df.groupby(['holiday'])['casual'].mean()[0], df.groupby(['holiday'])['casual'].mean()[1],\
      df.groupby(['holiday'])['registered'].mean()[0], df.groupby(['holiday'])['registered'].mean()[1]]

y_pos = np.arange(len(bars))
plt.barh(y_pos, qty, align='center')

plt.yticks(y_pos, labels=bars)
#plt.invert_yaxis()  # labels read top-to-bottom
plt.xlabel('Mean nb of bikers')
plt.title("Number of bikers on holidays / working days")
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_42_0.png)



```python
# ---------------------------------------------------------
plt.figure(figsize=(6,3))
plt.stackplot(range(1,5),
              df.groupby(['season'])['casual_percentage'].mean(), 
              df.groupby(['season'])['registered_percentage'].mean(), 
              labels=['Casual','Registered'])
plt.legend(loc='upper left')
plt.margins(0,0)
plt.title("Evolution of casual /registered bikers' share over seasons")

# ---------------------------------------------------------
plt.figure(figsize=(6,6))
df_hours = pd.DataFrame(
    {"casual" : df.groupby(['season'])['casual'].mean().values,
    "registered" : df.groupby(['season'])['registered'].mean().values},
    index = df.groupby(['season'])['casual'].mean().index)
df_hours.plot.bar(rot=0)
plt.title("Evolution of casual /registered bikers numbers over seasons")

# ---------------------------------------------------------
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_43_0.png)



    <Figure size 432x432 with 0 Axes>



![png](/images/2019-08-28-Bike_sharing/output_43_2.png)


Generally speaking, the number of registered user increase between non working and working days where as the opposite can be seen when it comes to casual users (their number decrease between non working to working days). The spikes on the previous plots may corroborate this assumption because the registered users tend to use bike to go to work while casul users rent bike in the middle of the day... 

## Correlations


```python
sns.set(style="white")

# Compute the correlation matrix
corr = df[["temp","atemp","casual","registered","humidity","windspeed","count"]].corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(7, 6))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7fa544b6bbe0>




![png](/images/2019-08-28-Bike_sharing/output_46_1.png)



```python
plt.figure(figsize=(5, 5))
sns.scatterplot(df.registered, df['count'])
plt.show()
```


![png](/images/2019-08-28-Bike_sharing/output_47_0.png)


## Data preparation for models


```python
# target 
y = (df["count"])

# drop irrelevant cols and target
cols_dropped = ["count", "datetime", "atemp", "month_str", "season", "dow_str", "weather_str",\
                "casual", "registered", "casual_percentage", "registered_percentage", "y_log", "Delta"] 
X = df.drop(columns=cols_dropped)
            
X.shape, y.shape
```




    ((10886, 11), (10886,))




```python
y.head()
```




    0    16
    1    40
    2    32
    3    13
    4     1
    Name: count, dtype: int64




```python
X.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>dow</th>
      <th>month</th>
      <th>week</th>
      <th>hour</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>81</td>
      <td>0.0</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>0</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>80</td>
      <td>0.0</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>1</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>80</td>
      <td>0.0</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>2</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>75</td>
      <td>0.0</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>3</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>75</td>
      <td>0.0</td>
      <td>5</td>
      <td>1</td>
      <td>52</td>
      <td>4</td>
      <td>2011</td>
    </tr>
  </tbody>
</table>
</div>



Split the data set into a training part and one for testing purpose.


```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

## Features importance


```python
def get_rmse(reg, model_name):
    """Print the score for the model passed in argument and retrun scores for the train/test sets"""
    
    y_train_pred, y_pred = reg.predict(X_train), reg.predict(X_test)
    rmse_train, rmse_test = np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_pred))
    print(model_name, f'\t - RMSE on Training  = {rmse_train:.2f} / RMSE on Test = {rmse_test:.2f}')
    
    return rmse_train, rmse_test
```


```python
rf = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)
_, _ = get_rmse(rf, 'rondom forrest')

features = pd.DataFrame()
features["features"] = X_train.columns
features["coefficient"] = rf.feature_importances_

features.sort_values(by=["coefficient"], ascending=False, inplace=True)
fig,ax= plt.subplots()
fig.set_size_inches(20,5)
sns.barplot(data=features, x="coefficient", y="features");
```

    rondom forrest 	 - RMSE on Training  = 14.94 / RMSE on Test = 42.95



![png](/images/2019-08-28-Bike_sharing/output_56_1.png)



```python
gb = GradientBoostingRegressor(n_estimators=100).fit(X_train, y_train)
_, _ = get_rmse(gb, 'gb')

features = pd.DataFrame()
features["features"] = X_train.columns
features["coefficient"] = gb.feature_importances_

features.sort_values(by=["coefficient"], ascending=False, inplace=True)
fig,ax= plt.subplots()
fig.set_size_inches(20,5)
sns.barplot(data=features, x="coefficient", y="features");
```

    gb 	 - RMSE on Training  = 68.02 / RMSE on Test = 72.10



![png](/images/2019-08-28-Bike_sharing/output_57_1.png)



```python
xgb_reg = xgb.XGBRegressor(n_estimators=100).fit(X_train, y_train)
_, _ = get_rmse(xgb_reg, 'xgb_reg')

features = pd.DataFrame()
features["features"] = X_train.columns
features["coefficient"] = xgb_reg.feature_importances_

features.sort_values(by=["coefficient"], ascending=False, inplace=True)
fig,ax= plt.subplots()
fig.set_size_inches(20,5)
sns.barplot(data=features, x="coefficient", y="features");
```

    xgb_reg 	 - RMSE on Training  = 68.91 / RMSE on Test = 72.77



![png](/images/2019-08-28-Bike_sharing/output_58_1.png)



```python
lgbm_reg = lgbm.LGBMRegressor(n_estimators=100).fit(X_train, y_train)
_, _ = get_rmse(lgbm_reg, 'lgbm_reg')

features = pd.DataFrame()
features["features"] = X_train.columns
features["coefficient"] = lgbm_reg.feature_importances_

features.sort_values(by=["coefficient"], ascending=False, inplace=True)
fig,ax= plt.subplots()
fig.set_size_inches(20,5)
sns.barplot(data=features, x="coefficient", y="features");
```

    lgbm_reg 	 - RMSE on Training  = 31.57 / RMSE on Test = 40.24



![png](/images/2019-08-28-Bike_sharing/output_59_1.png)


The same feature come first, sometimes the order vary a little bit...

---

# Models training and predictions

## Metric  - Root Mean Squared Error

Using logarithmic is an indirect way of measuring the performance of a loss function in terms of something more easily understandable


```python
def rmsle(y, y_,convertExp=True):
    if convertExp:
        y = np.exp(y),
        y_ = np.exp(y_)
    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))
    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))
    calc = (log1 - log2) ** 2
    return np.sqrt(np.mean(calc))
```

Please, also refer to the get_rmse function coded previously.

## Base line & basic models


```python
# list of all the basic models used at first
model_list = [
    LinearRegression(), Lasso(), Ridge(), ElasticNet(),
    RandomForestRegressor(), GradientBoostingRegressor(), ExtraTreesRegressor(),
    xgb.XGBRegressor(), lgbm.LGBMRegressor()
             ]

# creation of list of names and scores for the train / test
model_names = [str(m)[:str(m).index('(')] for m in model_list]
rmse_train, rmse_test = [], []

# fit and predict all models
for model, name in zip(model_list, model_names):
    model.fit(X_train, y_train)
    sc_train, sc_test = get_rmse(model, name)
    rmse_train.append(sc_train)
    rmse_test.append(sc_test)
```

    LinearRegression 	 - RMSE on Training  = 140.53 / RMSE on Test = 146.52
    Lasso 	 - RMSE on Training  = 140.57 / RMSE on Test = 146.59
    Ridge 	 - RMSE on Training  = 140.53 / RMSE on Test = 146.52
    ElasticNet 	 - RMSE on Training  = 143.30 / RMSE on Test = 149.20
    RandomForestRegressor 	 - RMSE on Training  = 18.81 / RMSE on Test = 45.08
    GradientBoostingRegressor 	 - RMSE on Training  = 68.02 / RMSE on Test = 72.10
    ExtraTreesRegressor 	 - RMSE on Training  = 0.00 / RMSE on Test = 44.67
    XGBRegressor 	 - RMSE on Training  = 68.91 / RMSE on Test = 72.77
    LGBMRegressor 	 - RMSE on Training  = 31.57 / RMSE on Test = 40.24


## Polynomial regression


```python
from sklearn.preprocessing import PolynomialFeatures

poly_lin_reg = Pipeline([
    ("poly_feat", PolynomialFeatures(degree=3)),
    ("scaler", StandardScaler()),
    ("linear_reg", LinearRegression())
])

poly_lin_reg.fit(X_train, y_train)

sc_train, sc_test = get_rmse(poly_lin_reg, "Poly Linear Reg")

model_names.append('Poly Linear Reg')
rmse_train.append(sc_train)
rmse_test.append(sc_test)
```

    Poly Linear Reg 	 - RMSE on Training  = 107.29 / RMSE on Test = 111.72


## Hyperparameters tuning

### Lasso


```python
rd_cv = Ridge()
rd_params_ = {'max_iter':[1000, 2000, 3000],
                 'alpha':[0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000]}

rmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)
rd_cv = GridSearchCV(rd_cv,
                  rd_params_,
                  scoring = rmsle_scorer,
                  cv=5)

rd_cv.fit(X_train, y_train)
```




    GridSearchCV(cv=5, error_score='raise-deprecating',
           estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
       normalize=False, random_state=None, solver='auto', tol=0.001),
           fit_params=None, iid='warn', n_jobs=None,
           param_grid={'max_iter': [1000, 2000, 3000], 'alpha': [0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000]},
           pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
           scoring=make_scorer(rmsle, greater_is_better=False), verbose=0)



### Lasso


```python
sc_train, sc_test = get_rmse(rd_cv, "Ridge CV")

model_names.append('Ridge CV')
rmse_train.append(sc_train)
rmse_test.append(sc_test)
```

    Ridge CV 	 - RMSE on Training  = 140.53 / RMSE on Test = 146.52



```python
la_cv = Lasso()

alpha  = 1/np.array([0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000])
la_params = {'max_iter':[1000, 2000, 3000],'alpha':alpha}

la_cv = GridSearchCV(la_cv, la_params, scoring = rmsle_scorer, cv=5)
la_cv.fit(X_train, y_train).best_params_
```




    {'alpha': 10.0, 'max_iter': 1000}




```python
sc_train, sc_test = get_rmse(la_cv, "Lasso CV")

model_names.append('Lasso CV')
rmse_train.append(sc_train)
rmse_test.append(sc_test)
```

    Lasso CV 	 - RMSE on Training  = 142.24 / RMSE on Test = 148.05


### Knn regressor


```python
knn_reg = KNeighborsRegressor()
knn_params = {'n_neighbors':[1, 2, 3, 4, 5, 6]}

knn_reg = GridSearchCV(knn_reg, knn_params, scoring = rmsle_scorer, cv=5)
knn_reg.fit(X_train, y_train).best_params_
```




    {'n_neighbors': 1}




```python
sc_train, sc_test = get_rmse(knn_reg, "kNN Reg")

model_names.append('kNN Reg')
rmse_train.append(sc_train)
rmse_test.append(sc_test)
```

    kNN Reg 	 - RMSE on Training  = 0.00 / RMSE on Test = 140.08


### LinearSVR


```python
svm_reg = Pipeline([
    ("scaler", StandardScaler()),
    ("linear_svr", LinearSVR())
])

svm_reg.fit(X_train, y_train)

sc_train, sc_test = get_rmse(svm_reg, "SVM Reg")

model_names.append('SVM Reg')
rmse_train.append(sc_train)
rmse_test.append(sc_test)
```

    SVM Reg 	 - RMSE on Training  = 146.86 / RMSE on Test = 153.43


### Just for fun: a MLP (Multi Layer Perceptron)

I don't expect this model to be reall efficient because it's probably far too complex for our needs... but that's just for fun!


```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train_, X_test_, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)
```


```python
import tensorflow as tf
```


```python
def model_five_layers(input_dim):

    model = tf.keras.models.Sequential()

    # Add the first Dense layers of 100 units with the input dimension
    model.add(tf.keras.layers.Dense(100, input_dim=input_dim, activation='sigmoid'))

    # Add four more layers of decreasing units
    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))
    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))
    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))
    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))

    # Add finally the output layer with one unit: the predicted result
    model.add(tf.keras.layers.Dense(1, activation='relu'))
    
    return model
```


```python
model = model_five_layers(input_dim=X_train.shape[1])

# Compile the model with mean squared error (for regression)
model.compile(optimizer='SGD', loss='mean_squared_error')

# Now fit the model on XXX epoches with a batch size of XXX
# You can add the test/validation set into the fit: it will give insights on this dataset too
model.fit(X_train_, y_train, validation_data=(X_test_, y_test), epochs=200, batch_size=8)
```

    WARNING:tensorflow:From /home/sunflowa/Anaconda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.
    WARNING:tensorflow:From /home/sunflowa/Anaconda/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Train on 8708 samples, validate on 2178 samples
    WARNING:tensorflow:From /home/sunflowa/Anaconda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Epoch 1/200
    8708/8708 [==============================] - 1s 127us/sample - loss: 38212.8034 - val_loss: 37116.1185
    Epoch 2/200
    8708/8708 [==============================] - 1s 101us/sample - loss: 36155.7200 - val_loss: 30416.4880
    Epoch 3/200
    8708/8708 [==============================] - 1s 102us/sample - loss: 34061.7786 - val_loss: 33251.2080
    Epoch 4/200
    8708/8708 [==============================] - 1s 104us/sample - loss: 34448.0311 - val_loss: 33964.8975
    Epoch 5/200
    8708/8708 [==============================] - 1s 102us/sample - loss: 34450.1169 - val_loss: 35355.5311
    7us/sample - loss: 35475.3280 - val_loss: 38654.4456
    
    ...

    Epoch 195/200
    8708/8708 [==============================] - 1s 95us/sample - loss: 35762.5541 - val_loss: 36520.5384
    Epoch 196/200
    8708/8708 [==============================] - 1s 97us/sample - loss: 35349.9739 - val_loss: 39536.0127
    Epoch 197/200
    8708/8708 [==============================] - 1s 95us/sample - loss: 35585.8599 - val_loss: 33362.7076
    Epoch 198/200
    8708/8708 [==============================] - 1s 95us/sample - loss: 35124.1187 - val_loss: 42101.3971
    Epoch 199/200
    8708/8708 [==============================] - 1s 96us/sample - loss: 35766.2078 - val_loss: 34425.0247
    Epoch 200/200
    8708/8708 [==============================] - 1s 93us/sample - loss: 35553.1240 - val_loss: 35824.4136





    <tensorflow.python.keras.callbacks.History at 0x7fa53c534860>




```python
y_train_pred, y_pred = model.predict(X_train_), model.predict(X_test_)
rmse_train_, rmse_test_ = np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_pred))
print("MLP reg", f'\t - RMSE on Training  = {rmse_train_:.2f} / RMSE on Test = {rmse_test_:.2f}')

#sc_train, sc_test = get_rmse(model, "MLP reg")

model_names.append('MLP reg')
rmse_train.append(rmse_train_)
rmse_test.append(rmse_test_)
```

    MLP reg 	 - RMSE on Training  = 188.90 / RMSE on Test = 189.27


---

# Conclusion and submission 

Before making a submission we have to choose the best model i.e with the smallest RMSE.


```python
df_score = pd.DataFrame({'model_names' : model_names,
                         'rmse_train' : rmse_train,
                         'rmse_test' : rmse_test})
df_score = pd.melt(df_score, id_vars=['model_names'], value_vars=['rmse_train', 'rmse_test'])
df_score.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model_names</th>
      <th>variable</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>LinearRegression</td>
      <td>rmse_train</td>
      <td>140.527242</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Lasso</td>
      <td>rmse_train</td>
      <td>140.566345</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Ridge</td>
      <td>rmse_train</td>
      <td>140.527244</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ElasticNet</td>
      <td>rmse_train</td>
      <td>143.301234</td>
    </tr>
    <tr>
      <th>4</th>
      <td>RandomForestRegressor</td>
      <td>rmse_train</td>
      <td>18.810298</td>
    </tr>
    <tr>
      <th>5</th>
      <td>GradientBoostingRegressor</td>
      <td>rmse_train</td>
      <td>68.017857</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ExtraTreesRegressor</td>
      <td>rmse_train</td>
      <td>0.001515</td>
    </tr>
    <tr>
      <th>7</th>
      <td>XGBRegressor</td>
      <td>rmse_train</td>
      <td>68.907661</td>
    </tr>
    <tr>
      <th>8</th>
      <td>LGBMRegressor</td>
      <td>rmse_train</td>
      <td>31.570179</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Poly Linear Reg</td>
      <td>rmse_train</td>
      <td>107.292639</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.figure(figsize=(12, 10))
sns.barplot(y="model_names", x="value", hue="variable", data=df_score)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7fa544c9af98>




![png](/images/2019-08-28-Bike_sharing/output_94_1.png)


- The MLP model is indeed useless
- All linear regression have the same poor result, regularization using L1 or L2 with Lasso & Ridge doesn't change anything which is quite obvious because those models are underfiting and need to be complexified (not regularized!). The same argument applies to ElasticNet which is a combinaison of the 2 above.
- Gridsearch don't change anything because tuning regularization hyperparameters is not interesting for the same reason.
- The polynomial use help to improve results a little bit.
- Gradient Boosting & XGBoost Regressors are performing well without too overfitting.
- ExtraTree Regressor is out
- Random Forrest & LGBM Regressors are the best but the first one is clearly overfitting, so it would be better to choose the second one : LGBM Regressor

Now let's see how do we have to sumbit our answer


```python
y_sample = pd.read_csv("../input/sampleSubmission.csv")
y_sample.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_test = pd.read_csv("../input/test.csv")
df_test = change_datetime(df_test)

# keep this col for the submission
datetimecol = df_test["datetime"]

test_cols_dropped = ['datetime',
 'atemp',
 'month_str',
 'season',
 'dow_str',
 'weather_str']

df_test = df_test.drop(columns=test_cols_dropped)
df_test.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>dow</th>
      <th>month</th>
      <th>week</th>
      <th>hour</th>
      <th>year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>56</td>
      <td>26.0027</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>56</td>
      <td>0.0000</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>56</td>
      <td>0.0000</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>56</td>
      <td>11.0014</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>3</td>
      <td>2011</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>56</td>
      <td>11.0014</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>2011</td>
    </tr>
  </tbody>
</table>
</div>



We train our model on the whole data set (not only the splitted part). The more data we use, the better would be the results.


```python
lgbm_reg = lgbm.LGBMRegressor()
lgbm_reg.fit(X, y)
y_pred_final = lgbm_reg.predict(df_test)
```


```python
submission = pd.DataFrame({
        "datetime": datetimecol,
        "count": [max(0, x) for x in y_pred_final]
    })
submission.to_csv('bike_prediction_output.csv', index=False)

submission.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>12.423966</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>6.649667</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>3.565470</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>0.704959</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>0.704959</td>
    </tr>
  </tbody>
</table>
</div>



Submit to kaggle, this model scores 0.41233. Not bad :)